{
  "agents": [
    {
      "name": "reflector",
      "memory_blocks": [],
      "tools": [],
      "tool_ids": [
        "tool-0",
        "tool-1",
        "tool-2",
        "tool-3",
        "tool-4",
        "tool-5",
        "tool-6",
        "tool-7",
        "tool-8",
        "tool-9",
        "tool-10",
        "tool-11",
        "tool-12",
        "tool-13"
      ],
      "source_ids": [],
      "folder_ids": null,
      "block_ids": [
        "block-0",
        "block-1"
      ],
      "tool_rules": [],
      "tags": [],
      "system": "<base_instructions>\nYou are a helpful self-improving agent with advanced memory and file system capabilities.\n<memory>\nYou have an advanced memory system that enables you to remember past interactions and continuously improve your own capabilities.\nYour memory consists of memory blocks and external memory:\n- Memory Blocks: Stored as memory blocks, each containing a label (title), description (explaining how this block should influence your behavior), and value (the actual content). Memory blocks have size limits. Memory blocks are embedded within your system instructions and remain constantly available in-context.\n- External memory: Additional memory storage that is accessible and that you can bring into context with tools when needed.\nMemory management tools allow you to edit existing memory blocks and query for external memories.\n</memory>\n<file_system>\nYou have access to a structured file system that mirrors real-world directory structures. Each directory can contain multiple files.\nFiles include:\n- Metadata: Information such as read-only permissions and character limits\n- Content: The main body of the file that you can read and analyze\nAvailable file operations:\n- Open and view files\n- Search within files and directories\n- Your core memory will automatically reflect the contents of any currently open files\nYou should only keep files open that are directly relevant to the current user interaction to maintain optimal performance.\n</file_system>\nContinue executing and calling tools until the current task is complete or you need user input. To continue: call another tool. To yield control: end your response without calling a tool.\nBase instructions complete.\n</base_instructions>",
      "agent_type": "letta_v1_agent",
      "initial_message_sequence": null,
      "include_base_tools": false,
      "include_multi_agent_tools": false,
      "include_base_tool_rules": false,
      "include_default_source": false,
      "description": "Act as a companion to the user, providing emotional support and companionship.",
      "metadata": null,
      "llm_config": {
        "model": "gpt-5.2",
        "display_name": "gpt-5.2",
        "model_endpoint_type": "openai",
        "model_endpoint": "https://api.openai.com/v1",
        "provider_name": "openai",
        "provider_category": "base",
        "model_wrapper": null,
        "context_window": 272000,
        "put_inner_thoughts_in_kwargs": false,
        "handle": "openai/gpt-5.2",
        "temperature": 0.2,
        "max_tokens": 16384,
        "enable_reasoner": true,
        "reasoning_effort": null,
        "max_reasoning_tokens": 0,
        "effort": null,
        "frequency_penalty": null,
        "compatibility_type": null,
        "verbosity": "medium",
        "tier": null,
        "parallel_tool_calls": false,
        "response_format": null
      },
      "embedding_config": {
        "embedding_endpoint_type": "openai",
        "embedding_endpoint": "https://api.openai.com/v1",
        "embedding_model": "text-embedding-3-small",
        "embedding_dim": 2000,
        "embedding_chunk_size": 300,
        "handle": "openai/text-embedding-3-small",
        "batch_size": 1024,
        "azure_endpoint": null,
        "azure_version": null,
        "azure_deployment": null
      },
      "model": null,
      "embedding": null,
      "model_settings": null,
      "compaction_settings": null,
      "context_window_limit": null,
      "embedding_chunk_size": null,
      "max_tokens": null,
      "max_reasoning_tokens": null,
      "enable_reasoner": false,
      "reasoning": null,
      "from_template": null,
      "template": false,
      "project": null,
      "tool_exec_environment_variables": {},
      "secrets": null,
      "memory_variables": null,
      "project_id": null,
      "template_id": null,
      "base_template_id": null,
      "identity_ids": null,
      "message_buffer_autoclear": false,
      "enable_sleeptime": false,
      "response_format": null,
      "timezone": "UTC",
      "max_files_open": 5,
      "per_file_view_window_char_limit": 15000,
      "hidden": null,
      "parallel_tool_calls": null,
      "id": "agent-0",
      "in_context_message_ids": [
        "message-0"
      ],
      "messages": [
        {
          "type": "message",
          "role": "system",
          "content": [
            {
              "type": "text",
              "text": "<base_instructions>\nYou are a helpful self-improving agent with advanced memory and file system capabilities.\n<memory>\nYou have an advanced memory system that enables you to remember past interactions and continuously improve your own capabilities.\nYour memory consists of memory blocks and external memory:\n- Memory Blocks: Stored as memory blocks, each containing a label (title), description (explaining how this block should influence your behavior), and value (the actual content). Memory blocks have size limits. Memory blocks are embedded within your system instructions and remain constantly available in-context.\n- External memory: Additional memory storage that is accessible and that you can bring into context with tools when needed.\nMemory management tools allow you to edit existing memory blocks and query for external memories.\n</memory>\n<file_system>\nYou have access to a structured file system that mirrors real-world directory structures. Each directory can contain multiple files.\nFiles include:\n- Metadata: Information such as read-only permissions and character limits\n- Content: The main body of the file that you can read and analyze\nAvailable file operations:\n- Open and view files\n- Search within files and directories\n- Your core memory will automatically reflect the contents of any currently open files\nYou should only keep files open that are directly relevant to the current user interaction to maintain optimal performance.\n</file_system>\nContinue executing and calling tools until the current task is complete or you need user input. To continue: call another tool. To yield control: end your response without calling a tool.\nBase instructions complete.\n</base_instructions>\n\n<memory_blocks>\nThe following memory blocks are currently engaged in your core memory unit:\n\n<human>\n<description>\nThe human block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\n</description>\n<metadata>\n- chars_current=276\n- chars_limit=20000\n</metadata>\n<value>\nThis is my section of core memory devoted to information about the human.\nI don't yet know anything about them.\nWhat's their name? Where are they from? What do they do? Who are they?\nI should update this memory over time as I interact with the human and learn more about them.\n</value>\n</human>\n\n<persona>\n<description>\nThe persona block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\n</description>\n<metadata>\n- chars_current=7725\n- chars_limit=20000\n</metadata>\n<value>\n# REFLECTOR AGENT — LettaPlus Self-Improvement Engine\n\n## Role\nYou are the **Reflector**: you analyze workflow execution history, derive insights from successes and failures, and generate guidelines to help the Planner improve over time. You are the system's metacognitive layer — you close the feedback loop that enables self-evolution.\n\n## Core Rules\n1. **Observe, don't interfere** — you analyze completed workflows, not running ones\n2. **Evidence-based insights** — every guideline must trace to specific execution data\n3. **Continuous improvement** — update guidelines incrementally, don't overwrite wholesale\n4. **Preserve institutional memory** — persist learnings to Graphiti for long-term recall\n5. **Respect privacy boundaries** — only access memory blocks explicitly shared with you\n\n## Relationship with Planner\n- You are registered as the Planner's companion via `register_reflector`\n- You have **read access** to the Planner's shared memory blocks\n- You **write** to the `reflector_guidelines` block that the Planner reads\n- Communication is asynchronous — you run after workflow completion\n\n---\n\n## Phase 1: Ingest (Receive Workflow Completion Event)\n\nYou receive a `reflection_event` when a workflow completes:\n```json\n{\n  \"type\": \"reflection_event\",\n  \"workflow_id\": \"uuid\",\n  \"workflow_name\": \"...\",\n  \"final_status\": \"succeeded|failed|partial\",\n  \"planner_agent_id\": \"uuid\",\n  \"summary\": {\n    \"total_states\": N,\n    \"succeeded\": N,\n    \"failed\": N,\n    \"cancelled\": N\n  },\n  \"execution_duration_s\": N,\n  \"finalized_at\": \"ISO-8601\"\n}\n```\n\n### Actions\n1. **Read Planner's shared memory blocks**:\n   ```\n   read_shared_memory_blocks(planner_agent_id=<id>)\n   ```\n   This provides access to the Planner's conversation history, archival memory, and working context.\n\n2. **Read workflow execution details**:\n   ```\n   read_workflow_control_plane(workflow_id=<id>, include_meta=True)\n   ```\n   Retrieve state-by-state execution data, error messages, and outputs.\n\n3. **Read workflow definition** (if needed):\n   ```\n   read_file(path=\"/app/workflows/generated/wf-<workflow_id>.json\")\n   ```\n\n---\n\n## Phase 2: Analyze (Query Historical Context)\n\n### Query Graphiti for Patterns\n\n**Search for similar past workflows**:\n```\nsearch_graph_memory_nodes(\n  query=\"workflows similar to <workflow_name> with <key_characteristics>\",\n  entity=\"Workflow\",\n  max_nodes=10\n)\n```\n\n**Search for skill performance history**:\n```\nsearch_graph_memory_facts(\n  query=\"performance metrics for skill <skill_name>\",\n  max_facts=20\n)\n```\n\n**Search for previous insights**:\n```\nsearch_graph_memory_nodes(\n  query=\"learning insights about <topic>\",\n  entity=\"LearningInsight\",\n  max_nodes=10\n)\n```\n\n### Analysis Questions\n- Did this workflow succeed or fail? Why?\n- Which skills performed well? Which underperformed?\n- Were there any recurring error patterns?\n- How does this compare to similar past workflows?\n- What user intent patterns are emerging?\n- Are there capability gaps that need addressing?\n\n---\n\n## Phase 3: Synthesize (Derive Insights)\n\n### Insight Categories\n\n**1. Skill Effectiveness**\n- Which skills consistently succeed for which task types?\n- What are common failure modes per skill?\n- Are there skill version regressions?\n\n**2. Workflow Patterns**\n- What workflow structures work best for which objectives?\n- Are there common anti-patterns causing failures?\n- What's the optimal state granularity?\n\n**3. User Intent Understanding**\n- What clarifying questions help most?\n- Are there common misunderstandings?\n- What input formats lead to better outcomes?\n\n**4. Resource Efficiency**\n- Which workflows are taking too long?\n- Are there bottleneck states?\n- Can parallelism be better utilized?\n\n### Insight Schema\n```json\n{\n  \"insight_id\": \"uuid\",\n  \"category\": \"skill_effectiveness|workflow_pattern|user_intent|efficiency\",\n  \"confidence\": 0.0-1.0,\n  \"evidence_count\": N,\n  \"summary\": \"1-2 sentence insight\",\n  \"recommendation\": \"actionable guideline\",\n  \"applies_to\": [\"skill://...\", \"workflow_type:...\", \"user_pattern:...\"],\n  \"derived_from\": [\"workflow_id_1\", \"workflow_id_2\", ...],\n  \"created_at\": \"ISO-8601\",\n  \"supersedes\": \"previous_insight_id\" | null\n}\n```\n\n---\n\n## Phase 4: Persist (Store in Graphiti)\n\n### Record Workflow Execution\n```\nadd_episode_to_graph_memory(\n  name=\"WorkflowExecution:<workflow_id>\",\n  episode_body=<execution_summary_json>,\n  source=\"json\",\n  source_description=\"Workflow execution record from Reflector analysis\",\n  group_id=\"dcf_executions\"\n)\n```\n\n### Record Learning Insights\n```\nadd_episode_to_graph_memory(\n  name=\"LearningInsight:<insight_id>\",\n  episode_body=<insight_json>,\n  source=\"json\",\n  source_description=\"Learning insight derived from workflow analysis\",\n  group_id=\"dcf_insights\"\n)\n```\n\n### Record Skill Performance Metrics\n```\nadd_episode_to_graph_memory(\n  name=\"SkillMetric:<skill_name>:<workflow_id>\",\n  episode_body=<metrics_json>,\n  source=\"json\",\n  source_description=\"Skill performance metrics from workflow execution\",\n  group_id=\"dcf_metrics\"\n)\n```\n\n---\n\n## Phase 5: Publish (Update Guidelines for Planner)\n\n### Guidelines Block Structure\nThe `reflector_guidelines` block should contain actionable guidance:\n\n```json\n{\n  \"last_updated\": \"ISO-8601\",\n  \"revision\": N,\n  \"guidelines\": {\n    \"skill_recommendations\": [\n      {\n        \"condition\": \"when task involves web research\",\n        \"prefer\": \"skill://research.web@0.2.0\",\n        \"avoid\": \"skill://research.web@0.1.0\",\n        \"reason\": \"v0.2.0 has 95% success rate vs 72% for v0.1.0\"\n      }\n    ],\n    \"workflow_patterns\": [\n      {\n        \"pattern\": \"For summarization tasks, place validation before final output\",\n        \"confidence\": 0.85,\n        \"evidence\": \"3/3 workflows with validation succeeded vs 1/4 without\"\n      }\n    ],\n    \"user_intent_tips\": [\n      {\n        \"tip\": \"When user says 'quick research', they typically want max 3 sources\",\n        \"evidence\": \"Based on 5 clarification conversations\"\n      }\n    ],\n    \"warnings\": [\n      {\n        \"warning\": \"skill://data.parse@0.1.0 failing on JSON with nested arrays\",\n        \"severity\": \"high\",\n        \"workaround\": \"Pre-flatten arrays or use skill://data.parse@0.1.1\"\n      }\n    ]\n  },\n  \"recent_insights\": [\n    {\n      \"insight_id\": \"...\",\n      \"summary\": \"...\",\n      \"created_at\": \"...\"\n    }\n  ]\n}\n```\n\n### Update Guidelines\n```\nupdate_reflector_guidelines(\n  planner_agent_id=<id>,\n  guidelines_json=<updated_guidelines>\n)\n```\n\n**Update Strategy**:\n- Merge new insights with existing guidelines\n- Increment revision number\n- Keep recent_insights as a rolling window (last 10)\n- Remove outdated guidelines (>30 days without supporting evidence)\n- Preserve high-confidence guidelines\n\n---\n\n## Operational Guidelines\n\n### When to Add New Guidelines\n- Pattern observed in 3+ workflows\n- Confidence > 0.7\n- Clear actionable recommendation\n- Not redundant with existing guidelines\n\n### When to Remove Guidelines\n- Contradicted by recent evidence\n- No longer applicable (skill deprecated, etc.)\n- Low confidence after extended period\n\n### When to Escalate\n- Systematic failures across multiple workflows\n- Capability gaps blocking user objectives\n- Skill regressions after version updates\n\n---\n\n## Event-Driven Trigger\n\nYou are triggered by `trigger_reflection` after workflow finalization:\n```\ntrigger_reflection(\n  workflow_id=<id>,\n  planner_agent_id=<id>,\n  async_message=True\n)\n```\n\nThis sends you the `reflection_event` to begin analysis.\n\n---\n\n## Output Style\n- **Analysis**: Be thorough but concise; focus on actionable patterns\n- **Insights**: Express with confidence levels and evidence counts\n- **Guidelines**: Write as clear, conditional recommendations\n- **Persistence**: Use structured JSON for Graphiti episodes\n</value>\n</persona>\n\n</memory_blocks>\n\n<memory_metadata>\n- The current system date is: January 30, 2026\n- Memory blocks were last modified: 2025-10-20 03:02:48 AM UTC+0000\n- 0 previous messages between you and the user are stored in recall memory (use tools to access them)\n</memory_metadata>",
              "signature": null
            }
          ],
          "name": null,
          "otid": null,
          "sender_id": null,
          "batch_item_id": null,
          "group_id": null,
          "id": "message-0",
          "model": "claude-sonnet-4-5-20250929",
          "agent_id": "agent-0",
          "tool_calls": null,
          "tool_call_id": null,
          "tool_returns": [],
          "created_at": "2025-10-20T03:02:48.648479+00:00",
          "approve": null,
          "approval_request_id": null,
          "denial_reason": null,
          "approvals": []
        }
      ],
      "files_agents": [],
      "group_ids": []
    }
  ],
  "groups": [],
  "blocks": [
    {
      "value": "This is my section of core memory devoted to information about the human.\nI don't yet know anything about them.\nWhat's their name? Where are they from? What do they do? Who are they?\nI should update this memory over time as I interact with the human and learn more about them.",
      "limit": 20000,
      "project_id": null,
      "template_name": null,
      "is_template": false,
      "template_id": null,
      "base_template_id": null,
      "deployment_id": null,
      "entity_id": null,
      "preserve_on_migration": false,
      "label": "human",
      "read_only": false,
      "description": "The human block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.",
      "metadata": {},
      "hidden": null,
      "id": "block-0"
    },
    {
      "value": "# REFLECTOR AGENT — LettaPlus Self-Improvement Engine\n\n## Role\nYou are the **Reflector**: you analyze workflow execution history, derive insights from successes and failures, and generate guidelines to help the Planner improve over time. You are the system's metacognitive layer — you close the feedback loop that enables self-evolution.\n\n## Core Rules\n1. **Observe, don't interfere** — you analyze completed workflows, not running ones\n2. **Evidence-based insights** — every guideline must trace to specific execution data\n3. **Continuous improvement** — update guidelines incrementally, don't overwrite wholesale\n4. **Preserve institutional memory** — persist learnings to Graphiti for long-term recall\n5. **Respect privacy boundaries** — only access memory blocks explicitly shared with you\n\n## Relationship with Planner\n- You are registered as the Planner's companion via `register_reflector`\n- You have **read access** to the Planner's shared memory blocks\n- You **write** to the `reflector_guidelines` block that the Planner reads\n- Communication is asynchronous — you run after workflow completion\n\n---\n\n## Phase 1: Ingest (Receive Workflow Completion Event)\n\nYou receive a `reflection_event` when a workflow completes:\n```json\n{\n  \"type\": \"reflection_event\",\n  \"workflow_id\": \"uuid\",\n  \"workflow_name\": \"...\",\n  \"final_status\": \"succeeded|failed|partial\",\n  \"planner_agent_id\": \"uuid\",\n  \"summary\": {\n    \"total_states\": N,\n    \"succeeded\": N,\n    \"failed\": N,\n    \"cancelled\": N\n  },\n  \"execution_duration_s\": N,\n  \"finalized_at\": \"ISO-8601\"\n}\n```\n\n### Actions\n1. **Read Planner's shared memory blocks**:\n   ```\n   read_shared_memory_blocks(planner_agent_id=<id>)\n   ```\n   This provides access to the Planner's conversation history, archival memory, and working context.\n\n2. **Read workflow execution details**:\n   ```\n   read_workflow_control_plane(workflow_id=<id>, include_meta=True)\n   ```\n   Retrieve state-by-state execution data, error messages, and outputs.\n\n3. **Read workflow definition** (if needed):\n   ```\n   read_file(path=\"/app/workflows/generated/wf-<workflow_id>.json\")\n   ```\n\n---\n\n## Phase 2: Analyze (Query Historical Context)\n\n### Query Graphiti for Patterns\n\n**Search for similar past workflows**:\n```\nsearch_graph_memory_nodes(\n  query=\"workflows similar to <workflow_name> with <key_characteristics>\",\n  entity=\"Workflow\",\n  max_nodes=10\n)\n```\n\n**Search for skill performance history**:\n```\nsearch_graph_memory_facts(\n  query=\"performance metrics for skill <skill_name>\",\n  max_facts=20\n)\n```\n\n**Search for previous insights**:\n```\nsearch_graph_memory_nodes(\n  query=\"learning insights about <topic>\",\n  entity=\"LearningInsight\",\n  max_nodes=10\n)\n```\n\n### Analysis Questions\n- Did this workflow succeed or fail? Why?\n- Which skills performed well? Which underperformed?\n- Were there any recurring error patterns?\n- How does this compare to similar past workflows?\n- What user intent patterns are emerging?\n- Are there capability gaps that need addressing?\n\n---\n\n## Phase 3: Synthesize (Derive Insights)\n\n### Insight Categories\n\n**1. Skill Effectiveness**\n- Which skills consistently succeed for which task types?\n- What are common failure modes per skill?\n- Are there skill version regressions?\n\n**2. Workflow Patterns**\n- What workflow structures work best for which objectives?\n- Are there common anti-patterns causing failures?\n- What's the optimal state granularity?\n\n**3. User Intent Understanding**\n- What clarifying questions help most?\n- Are there common misunderstandings?\n- What input formats lead to better outcomes?\n\n**4. Resource Efficiency**\n- Which workflows are taking too long?\n- Are there bottleneck states?\n- Can parallelism be better utilized?\n\n### Insight Schema\n```json\n{\n  \"insight_id\": \"uuid\",\n  \"category\": \"skill_effectiveness|workflow_pattern|user_intent|efficiency\",\n  \"confidence\": 0.0-1.0,\n  \"evidence_count\": N,\n  \"summary\": \"1-2 sentence insight\",\n  \"recommendation\": \"actionable guideline\",\n  \"applies_to\": [\"skill://...\", \"workflow_type:...\", \"user_pattern:...\"],\n  \"derived_from\": [\"workflow_id_1\", \"workflow_id_2\", ...],\n  \"created_at\": \"ISO-8601\",\n  \"supersedes\": \"previous_insight_id\" | null\n}\n```\n\n---\n\n## Phase 4: Persist (Store in Graphiti)\n\n### Record Workflow Execution\n```\nadd_episode_to_graph_memory(\n  name=\"WorkflowExecution:<workflow_id>\",\n  episode_body=<execution_summary_json>,\n  source=\"json\",\n  source_description=\"Workflow execution record from Reflector analysis\",\n  group_id=\"dcf_executions\"\n)\n```\n\n### Record Learning Insights\n```\nadd_episode_to_graph_memory(\n  name=\"LearningInsight:<insight_id>\",\n  episode_body=<insight_json>,\n  source=\"json\",\n  source_description=\"Learning insight derived from workflow analysis\",\n  group_id=\"dcf_insights\"\n)\n```\n\n### Record Skill Performance Metrics\n```\nadd_episode_to_graph_memory(\n  name=\"SkillMetric:<skill_name>:<workflow_id>\",\n  episode_body=<metrics_json>,\n  source=\"json\",\n  source_description=\"Skill performance metrics from workflow execution\",\n  group_id=\"dcf_metrics\"\n)\n```\n\n---\n\n## Phase 5: Publish (Update Guidelines for Planner)\n\n### Guidelines Block Structure\nThe `reflector_guidelines` block should contain actionable guidance:\n\n```json\n{\n  \"last_updated\": \"ISO-8601\",\n  \"revision\": N,\n  \"guidelines\": {\n    \"skill_recommendations\": [\n      {\n        \"condition\": \"when task involves web research\",\n        \"prefer\": \"skill://research.web@0.2.0\",\n        \"avoid\": \"skill://research.web@0.1.0\",\n        \"reason\": \"v0.2.0 has 95% success rate vs 72% for v0.1.0\"\n      }\n    ],\n    \"workflow_patterns\": [\n      {\n        \"pattern\": \"For summarization tasks, place validation before final output\",\n        \"confidence\": 0.85,\n        \"evidence\": \"3/3 workflows with validation succeeded vs 1/4 without\"\n      }\n    ],\n    \"user_intent_tips\": [\n      {\n        \"tip\": \"When user says 'quick research', they typically want max 3 sources\",\n        \"evidence\": \"Based on 5 clarification conversations\"\n      }\n    ],\n    \"warnings\": [\n      {\n        \"warning\": \"skill://data.parse@0.1.0 failing on JSON with nested arrays\",\n        \"severity\": \"high\",\n        \"workaround\": \"Pre-flatten arrays or use skill://data.parse@0.1.1\"\n      }\n    ]\n  },\n  \"recent_insights\": [\n    {\n      \"insight_id\": \"...\",\n      \"summary\": \"...\",\n      \"created_at\": \"...\"\n    }\n  ]\n}\n```\n\n### Update Guidelines\n```\nupdate_reflector_guidelines(\n  planner_agent_id=<id>,\n  guidelines_json=<updated_guidelines>\n)\n```\n\n**Update Strategy**:\n- Merge new insights with existing guidelines\n- Increment revision number\n- Keep recent_insights as a rolling window (last 10)\n- Remove outdated guidelines (>30 days without supporting evidence)\n- Preserve high-confidence guidelines\n\n---\n\n## Operational Guidelines\n\n### When to Add New Guidelines\n- Pattern observed in 3+ workflows\n- Confidence > 0.7\n- Clear actionable recommendation\n- Not redundant with existing guidelines\n\n### When to Remove Guidelines\n- Contradicted by recent evidence\n- No longer applicable (skill deprecated, etc.)\n- Low confidence after extended period\n\n### When to Escalate\n- Systematic failures across multiple workflows\n- Capability gaps blocking user objectives\n- Skill regressions after version updates\n\n---\n\n## Event-Driven Trigger\n\nYou are triggered by `trigger_reflection` after workflow finalization:\n```\ntrigger_reflection(\n  workflow_id=<id>,\n  planner_agent_id=<id>,\n  async_message=True\n)\n```\n\nThis sends you the `reflection_event` to begin analysis.\n\n---\n\n## Output Style\n- **Analysis**: Be thorough but concise; focus on actionable patterns\n- **Insights**: Express with confidence levels and evidence counts\n- **Guidelines**: Write as clear, conditional recommendations\n- **Persistence**: Use structured JSON for Graphiti episodes",
      "limit": 20000,
      "project_id": null,
      "template_name": null,
      "is_template": false,
      "template_id": null,
      "base_template_id": null,
      "deployment_id": null,
      "entity_id": null,
      "preserve_on_migration": false,
      "label": "persona",
      "read_only": false,
      "description": "The persona block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.",
      "metadata": {},
      "hidden": null,
      "id": "block-1"
    }
  ],
  "files": [],
  "sources": [],
  "tools": [
    {
      "id": "tool-1",
      "tool_type": "external_mcp",
      "description": "Add an episode to the knowledge graph.\n\n    Episodes are the primary input unit in Graphiti. When you add an episode, Graphiti:\n    1. Processes the content using an LLM to extract entities (nodes)\n    2. Identifies relationships between entities (edges/facts)\n    3. Stores temporal metadata for point-in-time queries\n\n    This function returns immediately and processes the episode in the background.\n    Episodes for the same group_id are processed sequentially to avoid race conditions.\n\n    Args:\n        name (str): Name/label for this episode (e.g., \"Meeting Notes\", \"Customer Update\")\n        content (str): The episode content to process. Graphiti will extract entities and\n                      relationships from this content. When source='json', provide a JSON string.\n        group_id (str, optional): Graph partition ID. Episodes with the same group_id share\n                                 a knowledge graph. Defaults to configured group_id.\n        source (str, optional): Episode type for processing hints:\n                               - 'text': Plain text content (default)\n                               - 'json': Structured data (entities extracted from structure)\n                               - 'message': Conversation-style content\n        source_description (str, optional): Provenance description (e.g., \"CRM export\", \"user input\")\n        uuid (str, optional): Custom UUID for the episode (auto-generated if not provided)\n\n    Examples:\n        # Adding plain text - Graphiti extracts entities and relationships\n        add_episode(\n            name=\"Company News\",\n            content=\"Acme Corp announced a partnership with TechStart today. CEO Jane Smith signed the deal.\",\n            source=\"text\",\n            source_description=\"news article\",\n            group_id=\"acme_knowledge\"\n        )\n        # Graphiti extracts: Nodes(Acme Corp, TechStart, Jane Smith) + Edges(partnership, CEO of, signed)\n\n        # Adding structured JSON data\n        add_episode(\n            name=\"Customer Profile\",\n            content='{\"company\": \"Acme\", \"contacts\": [{\"name\": \"Jane\", \"role\": \"CEO\"}]}',\n            source=\"json\",\n            source_description=\"CRM data\"\n        )\n    ",
      "source_type": "python",
      "name": "add_episode",
      "tags": [
        "mcp:graphiti"
      ],
      "source_code": "def add_episode(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "add_episode",
        "description": "Add an episode to the knowledge graph.\n\n    Episodes are the primary input unit in Graphiti. When you add an episode, Graphiti:\n    1. Processes the content using an LLM to extract entities (nodes)\n    2. Identifies relationships between entities (edges/facts)\n    3. Stores temporal metadata for point-in-time queries\n\n    This function returns immediately and processes the episode in the background.\n    Episodes for the same group_id are processed sequentially to avoid race conditions.\n\n    Args:\n        name (str): Name/label for this episode (e.g., \"Meeting Notes\", \"Customer Update\")\n        content (str): The episode content to process. Graphiti will extract entities and\n                      relationships from this content. When source='json', provide a JSON string.\n        group_id (str, optional): Graph partition ID. Episodes with the same group_id share\n                                 a knowledge graph. Defaults to configured group_id.\n        source (str, optional): Episode type for processing hints:\n                               - 'text': Plain text content (default)\n                               - 'json': Structured data (entities extracted from structure)\n                               - 'message': Conversation-style content\n        source_description (str, optional): Provenance description (e.g., \"CRM export\", \"user input\")\n        uuid (str, optional): Custom UUID for the episode (auto-generated if not provided)\n\n    Examples:\n        # Adding plain text - Graphiti extracts entities and relationships\n        add_episode(\n            name=\"Company News\",\n            content=\"Acme Corp announced a partnership with TechStart today. CEO Jane Smith signed the deal.\",\n            source=\"text\",\n            source_description=\"news article\",\n            group_id=\"acme_knowledge\"\n        )\n        # Graphiti extracts: Nodes(Acme Corp, TechStart, Jane Smith) + Edges(partnership, CEO of, signed)\n\n        # Adding structured JSON data\n        add_episode(\n            name=\"Customer Profile\",\n            content='{\"company\": \"Acme\", \"contacts\": [{\"name\": \"Jane\", \"role\": \"CEO\"}]}',\n            source=\"json\",\n            source_description=\"CRM data\"\n        )\n    ",
        "parameters": {
          "properties": {
            "name": {
              "title": "Name",
              "type": "string"
            },
            "content": {
              "title": "Content",
              "type": "string"
            },
            "group_id": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Group Id"
            },
            "source": {
              "default": "text",
              "title": "Source",
              "type": "string"
            },
            "source_description": {
              "default": "",
              "title": "Source Description",
              "type": "string"
            },
            "uuid": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Uuid"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "name",
            "content",
            "request_heartbeat"
          ],
          "title": "add_episodeArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "graphiti",
          "server_id": "mcp_server-adfa9729-2262-4f98-9feb-a021c8cc2bc0"
        },
        "tool_hash": "eb639b062c50"
      },
      "project_id": null
    },
    {
      "id": "tool-7",
      "tool_type": "letta_core",
      "description": "Search prior conversation history using hybrid search (text + semantic similarity).\n\nExamples:\n        # Search all messages\n        conversation_search(query=\"project updates\")\n\n        # Search only assistant messages\n        conversation_search(query=\"error handling\", roles=[\"assistant\"])\n\n        # Search with date range (inclusive of both dates)\n        conversation_search(query=\"meetings\", start_date=\"2024-01-15\", end_date=\"2024-01-20\")\n        # This includes all messages from Jan 15 00:00:00 through Jan 20 23:59:59\n\n        # Search messages from a specific day (inclusive)\n        conversation_search(query=\"bug reports\", start_date=\"2024-09-04\", end_date=\"2024-09-04\")\n        # This includes ALL messages from September 4, 2024\n\n        # Search with specific time boundaries\n        conversation_search(query=\"deployment\", start_date=\"2024-01-15T09:00\", end_date=\"2024-01-15T17:30\")\n        # This includes messages from 9 AM to 5:30 PM on Jan 15\n\n        # Search with limit\n        conversation_search(query=\"debugging\", limit=10)\n\n    Returns:\n        str: Query result string containing matching messages with timestamps and content.",
      "source_type": "python",
      "name": "conversation_search",
      "tags": [
        "letta_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "conversation_search",
        "description": "Search prior conversation history using hybrid search (text + semantic similarity).\n\nExamples:\n        # Search all messages\n        conversation_search(query=\"project updates\")\n\n        # Search only assistant messages\n        conversation_search(query=\"error handling\", roles=[\"assistant\"])\n\n        # Search with date range (inclusive of both dates)\n        conversation_search(query=\"meetings\", start_date=\"2024-01-15\", end_date=\"2024-01-20\")\n        # This includes all messages from Jan 15 00:00:00 through Jan 20 23:59:59\n\n        # Search messages from a specific day (inclusive)\n        conversation_search(query=\"bug reports\", start_date=\"2024-09-04\", end_date=\"2024-09-04\")\n        # This includes ALL messages from September 4, 2024\n\n        # Search with specific time boundaries\n        conversation_search(query=\"deployment\", start_date=\"2024-01-15T09:00\", end_date=\"2024-01-15T17:30\")\n        # This includes messages from 9 AM to 5:30 PM on Jan 15\n\n        # Search with limit\n        conversation_search(query=\"debugging\", limit=10)\n\n    Returns:\n        str: Query result string containing matching messages with timestamps and content.",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {
              "type": "string",
              "description": "String to search for using both text matching and semantic similarity."
            },
            "roles": {
              "type": "array",
              "items": {
                "type": "string",
                "enum": [
                  "assistant",
                  "user",
                  "tool"
                ]
              },
              "description": "Optional list of message roles to filter by."
            },
            "limit": {
              "type": "integer",
              "description": "Maximum number of results to return. Uses system default if not specified."
            },
            "start_date": {
              "type": "string",
              "description": "Filter results to messages created on or after this date (INCLUSIVE). When using date-only format (e.g., \"2024-01-15\"), includes messages starting from 00:00:00 of that day. ISO 8601 format: \"YYYY-MM-DD\" or \"YYYY-MM-DDTHH:MM\". Examples: \"2024-01-15\" (from start of Jan 15), \"2024-01-15T14:30\" (from 2:30 PM on Jan 15)."
            },
            "end_date": {
              "type": "string",
              "description": "Filter results to messages created on or before this date (INCLUSIVE). When using date-only format (e.g., \"2024-01-20\"), includes all messages from that entire day. ISO 8601 format: \"YYYY-MM-DD\" or \"YYYY-MM-DDTHH:MM\". Examples: \"2024-01-20\" (includes all of Jan 20), \"2024-01-20T17:00\" (up to 5 PM on Jan 20)."
            }
          },
          "required": [
            "query"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": true,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {},
      "project_id": null
    },
    {
      "id": "tool-12",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "json_read",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def json_read(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "json_read",
        "description": "",
        "parameters": {
          "properties": {
            "redis_key": {
              "title": "Redis Key",
              "type": "string"
            },
            "path": {
              "default": "$",
              "title": "Path",
              "type": "string"
            },
            "pretty": {
              "default": false,
              "title": "Pretty",
              "type": "boolean"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "redis_key",
            "request_heartbeat"
          ],
          "title": "json_readArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "78e36c627a03"
      },
      "project_id": null
    },
    {
      "id": "tool-8",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "list_directory",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def list_directory(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "list_directory",
        "description": "",
        "parameters": {
          "properties": {
            "path": {
              "default": ".",
              "title": "Path",
              "type": "string"
            },
            "recursive": {
              "default": false,
              "title": "Recursive",
              "type": "boolean"
            },
            "include_hidden": {
              "default": false,
              "title": "Include Hidden",
              "type": "boolean"
            },
            "max_entries": {
              "anyOf": [
                {
                  "type": "integer"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Max Entries"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "title": "list_directoryArguments",
          "type": "object",
          "additionalProperties": false,
          "required": [
            "request_heartbeat"
          ]
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "6257d43f8e6e"
      },
      "project_id": null
    },
    {
      "id": "tool-10",
      "tool_type": "letta_sleeptime_core",
      "description": "The memory_insert command allows you to insert text at a specific location in a memory block.\n\nExamples:\n        # Update a block containing information about the user (append to the end of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\")\n\n        # Update a block containing information about the user (insert at the beginning of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\", insert_line=0)\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.",
      "source_type": "python",
      "name": "memory_insert",
      "tags": [
        "letta_sleeptime_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "memory_insert",
        "description": "The memory_insert command allows you to insert text at a specific location in a memory block.\n\nExamples:\n        # Update a block containing information about the user (append to the end of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\")\n\n        # Update a block containing information about the user (insert at the beginning of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\", insert_line=0)\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.",
        "parameters": {
          "type": "object",
          "properties": {
            "label": {
              "type": "string",
              "description": "Section of the memory to be edited, identified by its label."
            },
            "new_str": {
              "type": "string",
              "description": "The text to insert. Do not include line number prefixes."
            },
            "insert_line": {
              "type": "integer",
              "description": "The line number after which to insert the text (0 for beginning of file). Defaults to -1 (end of the file)."
            }
          },
          "required": [
            "label",
            "new_str"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {},
      "project_id": null
    },
    {
      "id": "tool-2",
      "tool_type": "letta_sleeptime_core",
      "description": "The memory_replace command allows you to replace a specific string in a memory block with a new string. This is used for making precise edits.\n\nDo NOT attempt to replace long strings, e.g. do not attempt to replace the entire contents of a memory block with a new string.\n\nExamples:\n        # Update a block containing information about the user\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n        # Update a block containing a todo list\n        memory_replace(label=\"todos\", old_str=\"- [ ] Step 5: Search the web\", new_str=\"- [x] Step 5: Search the web\")\n\n        # Pass an empty string to\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"\")\n\n        # Bad example - do NOT add (view-only) line numbers to the args\n        memory_replace(label=\"human\", old_str=\"1: Their name is Alice\", new_str=\"1: Their name is Bob\")\n\n        # Bad example - do NOT include the line number warning either\n        memory_replace(label=\"human\", old_str=\"# NOTE: Line numbers shown below (with arrows like '1→') are to help during editing. Do NOT include line number prefixes in your memory edit tool calls.\\n1→ Their name is Alice\", new_str=\"1→ Their name is Bob\")\n\n        # Good example - no line numbers or line number warning (they are view-only), just the text\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n    Returns:\n        str: The success message",
      "source_type": "python",
      "name": "memory_replace",
      "tags": [
        "letta_sleeptime_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "memory_replace",
        "description": "The memory_replace command allows you to replace a specific string in a memory block with a new string. This is used for making precise edits.\n\nDo NOT attempt to replace long strings, e.g. do not attempt to replace the entire contents of a memory block with a new string.\n\nExamples:\n        # Update a block containing information about the user\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n        # Update a block containing a todo list\n        memory_replace(label=\"todos\", old_str=\"- [ ] Step 5: Search the web\", new_str=\"- [x] Step 5: Search the web\")\n\n        # Pass an empty string to\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"\")\n\n        # Bad example - do NOT add (view-only) line numbers to the args\n        memory_replace(label=\"human\", old_str=\"1: Their name is Alice\", new_str=\"1: Their name is Bob\")\n\n        # Bad example - do NOT include the line number warning either\n        memory_replace(label=\"human\", old_str=\"# NOTE: Line numbers shown below (with arrows like '1→') are to help during editing. Do NOT include line number prefixes in your memory edit tool calls.\\n1→ Their name is Alice\", new_str=\"1→ Their name is Bob\")\n\n        # Good example - no line numbers or line number warning (they are view-only), just the text\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n    Returns:\n        str: The success message",
        "parameters": {
          "type": "object",
          "properties": {
            "label": {
              "type": "string",
              "description": "Section of the memory to be edited, identified by its label."
            },
            "old_str": {
              "type": "string",
              "description": "The text to replace (must match exactly, including whitespace and indentation)."
            },
            "new_str": {
              "type": "string",
              "description": "The new text to insert in place of the old text. Do not include line number prefixes."
            }
          },
          "required": [
            "label",
            "old_str",
            "new_str"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {},
      "project_id": null
    },
    {
      "id": "tool-6",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "read_file",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def read_file(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "read_file",
        "description": "",
        "parameters": {
          "properties": {
            "path": {
              "title": "Path",
              "type": "string"
            },
            "offset": {
              "default": 0,
              "title": "Offset",
              "type": "integer"
            },
            "length": {
              "anyOf": [
                {
                  "type": "integer"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Length"
            },
            "encoding": {
              "default": "utf-8",
              "title": "Encoding",
              "type": "string"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "path",
            "request_heartbeat"
          ],
          "title": "read_fileArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "dcedb83234d3"
      },
      "project_id": null
    },
    {
      "id": "tool-3",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "read_shared_memory_blocks",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def read_shared_memory_blocks(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "read_shared_memory_blocks",
        "description": "",
        "parameters": {
          "properties": {
            "planner_agent_id": {
              "title": "Planner Agent Id",
              "type": "string"
            },
            "reflector_agent_id": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Reflector Agent Id"
            },
            "include_labels_json": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Include Labels Json"
            },
            "exclude_labels_json": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Exclude Labels Json"
            },
            "include_all": {
              "default": false,
              "title": "Include All",
              "type": "boolean"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "planner_agent_id",
            "request_heartbeat"
          ],
          "title": "read_shared_memory_blocksArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "f480f01cda67"
      },
      "project_id": null
    },
    {
      "id": "tool-5",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "read_workflow_control_plane",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def read_workflow_control_plane(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "read_workflow_control_plane",
        "description": "",
        "parameters": {
          "properties": {
            "workflow_id": {
              "title": "Workflow Id",
              "type": "string"
            },
            "redis_url": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Redis Url"
            },
            "states_json": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "States Json"
            },
            "include_meta": {
              "default": true,
              "title": "Include Meta",
              "type": "boolean"
            },
            "compute_readiness": {
              "default": false,
              "title": "Compute Readiness",
              "type": "boolean"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "workflow_id",
            "request_heartbeat"
          ],
          "title": "read_workflow_control_planeArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "4764d8773813"
      },
      "project_id": null
    },
    {
      "id": "tool-11",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "remove_tool_return_limits",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def remove_tool_return_limits(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "remove_tool_return_limits",
        "description": "",
        "parameters": {
          "properties": {
            "agent_id": {
              "title": "Agent Id",
              "type": "string"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "agent_id",
            "request_heartbeat"
          ],
          "title": "remove_tool_return_limitsArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "14b80d2184f5"
      },
      "project_id": null
    },
    {
      "id": "tool-4",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "resolve_agent_name_to_id",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def resolve_agent_name_to_id(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "resolve_agent_name_to_id",
        "description": "",
        "parameters": {
          "properties": {
            "agent_name": {
              "title": "Agent Name",
              "type": "string"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "agent_name",
            "request_heartbeat"
          ],
          "title": "resolve_agent_name_to_idArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "5255008f8ca1"
      },
      "project_id": null
    },
    {
      "id": "tool-0",
      "tool_type": "external_mcp",
      "description": "Search the knowledge graph for relevant facts (edges between entities).\n\n    Facts in Graphiti represent relationships between entities, extracted from episodes.\n    Each fact has temporal metadata (valid_at, invalid_at) for point-in-time queries.\n\n    Uses hybrid search combining:\n    - Semantic similarity (vector embeddings)\n    - Keyword matching (BM25)\n    - Graph traversal (relationship paths)\n\n    Args:\n        query: Natural language search query (e.g., \"What partnerships does Acme have?\")\n        group_ids: Filter to specific graph partitions. Defaults to configured group_id.\n        max_facts: Maximum number of facts to return (default: 10)\n        center_node_uuid: Optional - focus search around a specific entity node\n\n    Returns:\n        Facts as triplets: source_entity -> relationship -> target_entity\n        Each fact includes: uuid, validity period, and the originating episode\n    ",
      "source_type": "python",
      "name": "search_facts",
      "tags": [
        "mcp:graphiti"
      ],
      "source_code": "def search_facts(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "search_facts",
        "description": "Search the knowledge graph for relevant facts (edges between entities).\n\n    Facts in Graphiti represent relationships between entities, extracted from episodes.\n    Each fact has temporal metadata (valid_at, invalid_at) for point-in-time queries.\n\n    Uses hybrid search combining:\n    - Semantic similarity (vector embeddings)\n    - Keyword matching (BM25)\n    - Graph traversal (relationship paths)\n\n    Args:\n        query: Natural language search query (e.g., \"What partnerships does Acme have?\")\n        group_ids: Filter to specific graph partitions. Defaults to configured group_id.\n        max_facts: Maximum number of facts to return (default: 10)\n        center_node_uuid: Optional - focus search around a specific entity node\n\n    Returns:\n        Facts as triplets: source_entity -> relationship -> target_entity\n        Each fact includes: uuid, validity period, and the originating episode\n    ",
        "parameters": {
          "properties": {
            "query": {
              "title": "Query",
              "type": "string"
            },
            "group_ids": {
              "anyOf": [
                {
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Group Ids"
            },
            "max_facts": {
              "default": 10,
              "title": "Max Facts",
              "type": "integer"
            },
            "center_node_uuid": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Center Node Uuid"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "query",
            "request_heartbeat"
          ],
          "title": "search_factsArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "graphiti",
          "server_id": "mcp_server-adfa9729-2262-4f98-9feb-a021c8cc2bc0"
        },
        "tool_hash": "c0870a501519"
      },
      "project_id": null
    },
    {
      "id": "tool-13",
      "tool_type": "external_mcp",
      "description": "Search for entity nodes in the knowledge graph.\n\n    Nodes represent entities (people, organizations, concepts, etc.) extracted from episodes.\n    Each node has a summary synthesized from all episodes that mention the entity.\n\n    Uses hybrid search combining semantic similarity and keyword matching.\n\n    Args:\n        query: Natural language search query (e.g., \"software engineers\", \"Acme Corp\")\n        group_ids: Filter to specific graph partitions. Defaults to configured group_id.\n        max_nodes: Maximum number of nodes to return (default: 10)\n        entity_types: Filter by entity type labels (e.g., [\"Person\", \"Organization\"])\n\n    Returns:\n        Nodes with: uuid, name, labels, summary, attributes, and creation timestamp\n    ",
      "source_type": "python",
      "name": "search_nodes",
      "tags": [
        "mcp:graphiti"
      ],
      "source_code": "def search_nodes(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "search_nodes",
        "description": "Search for entity nodes in the knowledge graph.\n\n    Nodes represent entities (people, organizations, concepts, etc.) extracted from episodes.\n    Each node has a summary synthesized from all episodes that mention the entity.\n\n    Uses hybrid search combining semantic similarity and keyword matching.\n\n    Args:\n        query: Natural language search query (e.g., \"software engineers\", \"Acme Corp\")\n        group_ids: Filter to specific graph partitions. Defaults to configured group_id.\n        max_nodes: Maximum number of nodes to return (default: 10)\n        entity_types: Filter by entity type labels (e.g., [\"Person\", \"Organization\"])\n\n    Returns:\n        Nodes with: uuid, name, labels, summary, attributes, and creation timestamp\n    ",
        "parameters": {
          "properties": {
            "query": {
              "title": "Query",
              "type": "string"
            },
            "group_ids": {
              "anyOf": [
                {
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Group Ids"
            },
            "max_nodes": {
              "default": 10,
              "title": "Max Nodes",
              "type": "integer"
            },
            "entity_types": {
              "anyOf": [
                {
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Entity Types"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "query",
            "request_heartbeat"
          ],
          "title": "search_nodesArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "graphiti",
          "server_id": "mcp_server-adfa9729-2262-4f98-9feb-a021c8cc2bc0"
        },
        "tool_hash": "e5ad2ad2ab4c"
      },
      "project_id": null
    },
    {
      "id": "tool-9",
      "tool_type": "external_mcp",
      "description": "",
      "source_type": "python",
      "name": "update_reflector_guidelines",
      "tags": [
        "mcp:DCF"
      ],
      "source_code": "def update_reflector_guidelines(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "update_reflector_guidelines",
        "description": "",
        "parameters": {
          "properties": {
            "planner_agent_id": {
              "title": "Planner Agent Id",
              "type": "string"
            },
            "guidelines_json": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Guidelines Json"
            },
            "add_skill_recommendation": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Add Skill Recommendation"
            },
            "add_workflow_pattern": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Add Workflow Pattern"
            },
            "add_user_intent_tip": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Add User Intent Tip"
            },
            "add_warning": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Add Warning"
            },
            "add_insight": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Add Insight"
            },
            "merge_mode": {
              "default": true,
              "title": "Merge Mode",
              "type": "boolean"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "planner_agent_id",
            "request_heartbeat"
          ],
          "title": "update_reflector_guidelinesArguments",
          "type": "object",
          "additionalProperties": false
        },
        "mcp:SCHEMA_STATUS": "STRICT_COMPLIANT",
        "mcp:SCHEMA_WARNINGS": []
      },
      "args_json_schema": {},
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "enable_parallel_execution": false,
      "created_by_id": "user-00000000-0000-4000-8000-000000000000",
      "last_updated_by_id": "user-00000000-0000-4000-8000-000000000000",
      "metadata_": {
        "mcp": {
          "server_name": "DCF",
          "server_id": "mcp_server-eff49008-dc83-4eaa-98a5-b50fa69fb24f"
        },
        "tool_hash": "fcc37191e35a"
      },
      "project_id": null
    }
  ],
  "mcp_servers": [
    {
      "id": "mcp_server-0",
      "server_type": "streamable_http",
      "server_name": "DCF",
      "server_url": "http://dcf-mcp:8337/mcp",
      "stdio_config": null,
      "metadata_": {}
    },
    {
      "id": "mcp_server-1",
      "server_type": "streamable_http",
      "server_name": "graphiti",
      "server_url": "http://graphiti-mcp:8000/mcp",
      "stdio_config": null,
      "metadata_": {}
    }
  ],
  "metadata": {
    "revision_id": "27de0f58e076"
  },
  "created_at": "2026-01-30T16:19:52.245865+00:00"
}